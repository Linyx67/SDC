import cv2
import numpy as np
import math
import csv
import time

from topView import topView, highlightView, transformPoints
from database import Report

def norm(x1, y1, x2, y2):
    """ 
    Calculate the distance between two points (x1, y1) and (x2, y2)
    in a 2D plane.
    """
    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)


def initializeYolo(labelsPath, configPath, weightsPath):
    """
    Detect objects in an image using a COCO dataset and a pre-trained
    YOLO object detection model. Paths to the COCO labels, YOLO 
    configuration, and YOLO weights files must be provided.
    """

    # Extract the labels from the COCO dataset file
    with open(labelsPath) as infile:
        labels = infile.read().rstrip('\n').split('\n')

    # Use OpenCV to read and process the network
    net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)

    # Get Layer Names and return them
    layers = net.getLayerNames()
    layers = [layers[i[0] - 1] for i in net.getUnconnectedOutLayers()]

    return layers, net, labels

def forwardImage(image, layers, net):
    # Make a blob from the input image
    blob = cv2.dnn.blobFromImage(image, 1.0 / 255.0,
                                 (416, 416), swapRB = True,
                                 crop = False)
    # Forward the layers to OpenCv
    net.setInput(blob)
    layerOutputs = net.forward(layers)

    return layerOutputs

def personDetect(image, layerOutputs, labels, confidenceLevel = 0.8):
    """
    Use layers generated by the detection model to detect persons
    in an image.
    """
    (H, W) = image.shape[:2]
    # Stores the boundary boxes, classification probability,
    # and class identifiers
    boxes = []
    confs = []
    #classIds = []

    # Iterate over each output layer and each object detected
    for output in layerOutputs:
        for detection in output:
            scores = detection[5:]
            id = np.argmax(scores)
            confidence = scores[id]

            # Ignore detections below the input confidence level
            # Ignore objects other than persons
            if labels[id] == "person" and confidence > confidenceLevel:
                # Scale the box to the image width and height
                box = detection[0:4] * np.array([W, H, W, H])
                # OpenCV gets the center point of the object detected
                # and its width and height
                (centerX, centerY, width, height) = box.astype("int")
                # (x, y) is the top left corner of the box
                x = int(centerX - (width / 2))
                y = int(centerY - (height / 2))

                boxes.append([x, y, int(width), int(height)])
                confs.append(float(confidence))
                #classIds.append(id)

    # Remove overlapping boxes
    indexes = cv2.dnn.NMSBoxes(boxes, confs, confidenceLevel,
                               confidenceLevel)
    return indexes, boxes, confs


def highlightPerson(image, point, lengths, color, thickness,
                     label, confidence):
    """
    Draw a rectangle around a detected person and display the confidence
    of the detection above the rectangle.
    """
    cv2.rectangle(image, point, lengths, color, thickness)
    text = "{}: {:.2f}".format(label, confidence * 100)
    cv2.putText(image, text, (point[0], point[1] - 5),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
    

def annotateImage(image, indexes, boxes, confs, safeDistance, region, reportObject, yearZero):
    """
    Iterate through each person detected and draw a green boxe around 
    them if they are within a safe distance, or a red box if not.
    Print the number of persons, the estimated crowd density, and the
    number of violations detected.
    """
    if len(indexes) > 0:
        topCorners = []
        boxLengths = []
        # Points on the ground directly below persons and their top view
        # transformations
        basePoints = []
        transformedPoints = []
        for i in indexes.flatten():
            # Get the dimensions for the boxes
            (x, y) = (boxes[i][0], boxes[i][1])
            (w, h) = (boxes[i][2], boxes[i][3])
            footPoint = (x + w//2, y + h)
            topCorners.append((x, y))
            boxLengths.append((w, h))
            basePoints.append((footPoint))

        # Transform the image to a top view to perform distance calculations
        view, transform = topView(image, region)
        transformedPoints = transformPoints(basePoints, transform)

        # There is a corner for each person detected
        personCount = len(topCorners)
        currentTime = time.perf_counter()

        for i in range(personCount):
            (x, y) = topCorners[i]
            (w, h) = boxLengths[i]
            violationCounter = 0
            
            for j in range(personCount):
                # Check the distance to every other person detected,
                # except the current person
                if j != i:
                    distance = norm(transformedPoints[i][0],
                                    transformedPoints[i][1],
                                    transformedPoints[j][0],
                                    transformedPoints[j][1])
                    if (distance < safeDistance):
                        # person too close to current person, draw
                        # a red box around them
                        violationCounter += 1
                        highlightPerson(image, (x, y), (x + w, y + h),
                                        (0, 0, 255), 2, "person",
                                        confs[i])
                    else:
                        # draw a green box
                        highlightPerson(image, (x, y), (x + w, y + h),
                                        (0, 255, 0), 2, "person",
                                        confs[i])

        # Print the number of persons detected the frame
        (H, W) = image.shape[:2]
        personCount = len(topCorners)
        personCountMessage = "Persons Detected: {}".format(
                personCount)
        cv2.putText(image, personCountMessage, (20, H - 90),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Print the crowd density in persons per pixel
        AREA = H * W
        crowdDensity = personCount / AREA
        crowdDensityMessage = \
            "Crowd Density: {:.2g} persons per pixel".format(
                crowdDensity)
        cv2.putText(image, crowdDensityMessage, (20, H - 60),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Print the number of violations detected
        violationMessage = \
            "Violation Count: {}".format(violationCounter)
        cv2.putText(image, violationMessage, (20, H - 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Populate report
        reportObject.writeLine(currentTime - yearZero, personCount, crowdDensity, violationCounter)
        
    return image

def runDetector(frame, layers, net, labels, safeDistance, region, reportObject, yearZero):
    """
    Apply the object detection system on an image to detect persons.
    The output image is annotated to highlight persons based on their
    proximity to other detected persons.
    """
    layerOutputs = forwardImage(frame, layers, net)
    indexes, boxes, confs = personDetect(frame, layerOutputs, labels, 0.5)
    annotateImage(frame, indexes, boxes, confs, safeDistance, region, reportObject, yearZero)
    return frame


def testDetector(filepath, windowTitle):
    layers, net, labels = initializeYolo("resources/coco.names",
                                          "resources/yolov4-tiny.cfg",
                                          "resources/yolov4-tiny.weights")
    region = ((299, 115), (748, 225), (512, 456), (52, 269))
    cap = cv2.VideoCapture(filepath)
    success, frame = cap.read()
    rep = Report("reports/" + windowTitle + ".csv")
    beginTime = time.perf_counter()
    while success:
        frame = runDetector(frame, layers, net, labels, 300, region, rep, beginTime)
        cv2.imshow(windowTitle, frame)
        success, frame = cap.read()
        if cv2.waitKey(1) == ord('q'):
            break
    cv2.destroyAllWindows()

def testDetector1():
    layers, net, labels = initializeYolo("resources/coco.names",
                                          "resources/yolov4-tiny.cfg",
                                          "resources/yolov4-tiny.weights")
    region = ((299, 115), (748, 225), (512, 456), (52, 269))
    cap = cv2.VideoCapture("resources/vtest.avi")
    success, frame = cap.read()
    rep = Report("reports/vtest_feed.csv")
    beginTime = time.perf_counter()
    while success:
        frame = runDetector(frame, layers, net, labels, 200, region, beginTime)
        cv2.imshow("Output", frame)
        success, frame = cap.read()
        if cv2.waitKey(1) == ord('q'):
            break
    cv2.destroyAllWindows()


def testDetector2():
    layers, net, labels = initializeYolo("resources/coco.names",
                                          "resources/yolov4-tiny.cfg",
                                          "resources/yolov4-tiny.weights")
    region = ((463, 72), (929, 153), (713, 536), (64, 296))
    cap = cv2.VideoCapture("resources/pedestrians.mkv")
    success, frame = cap.read()
    rep = Report("reports/pedestrians_feed.csv")
    beginTime = time.perf_counter()
    while success:
        frame = cv2.resize(frame, (960, 540))
        frame = runDetector(frame, layers, net, labels, 300, region, beginTime)
        cv2.imshow("Output", frame)
        success, frame = cap.read()
        if cv2.waitKey(1) == ord('q'):
            break
    cv2.destroyAllWindows()

# Uncomment to run some demos
#testDetector1()
#testDetector2()
